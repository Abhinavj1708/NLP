{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfdf6d29",
   "metadata": {},
   "source": [
    "## Details:17-02-2023\n",
    "\n",
    "## Topic: Vectorizing (tf-idf Vectorizer, CountVectorizer and Article similarity Checkup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417d9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96ffd0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\"NLP is awesome\",\"I want to start an NLP Company\",\"Top unicorns use NLP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a3ab149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(binary=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92090493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nlp': 4, 'is': 3, 'awesome': 1, 'want': 10, 'to': 6, 'start': 5, 'an': 0, 'company': 2, 'top': 7, 'unicorns': 8, 'use': 9}\n"
     ]
    }
   ],
   "source": [
    "vocab=vect.vocabulary_  #Attribute\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8fa787fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an:0\n",
      "awesome:1\n",
      "company:2\n",
      "is:3\n",
      "nlp:4\n",
      "start:5\n",
      "to:6\n",
      "top:7\n",
      "unicorns:8\n",
      "use:9\n",
      "want:10\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(vocab.keys()):\n",
    "    print(\"{}:{}\".format(key,vocab[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "571664a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform([\"I have an awesome NLP Company worth an awesome billion\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe42a5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform([\"ML is an elective subject\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6971ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform([\"NLP is an attractive career now\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8459556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(vect.transform([\"ML is an elective subject\"]).toarray(), vect.transform([\"NLP is an attractive career now\"]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1ba77ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81649658]]\n"
     ]
    }
   ],
   "source": [
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.machinelearningplus.com/nlp/cosine-similarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72f3fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra \n",
    "# FROM KAGGLE\n",
    "from scipy.spatial import distance\n",
    "def cosine_distance_countvectorizer_method(s1, s2):\n",
    "    \n",
    "    # sentences to list\n",
    "    allsentences = [s1 , s2]\n",
    " \n",
    "    # text to vector\n",
    "    vectorizer = CountVectorizer()\n",
    "    all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n",
    "    text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n",
    "    text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n",
    "    \n",
    "    # distance of similarity\n",
    "    cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n",
    "    print('Similarity of two sentences are equal to ',round((1-cosine)*100,2),'%')\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f66f341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of two sentences are equal to  36.51 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6348516283298893"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_distance_countvectorizer_method(\"ML is an elective subject\", \"NLP is an attractive career now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea349c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee4e6f04",
   "metadata": {},
   "source": [
    "## Tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7ec0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use Tfidf vectorizer instead of Count Vectorizer\n",
    "#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "#Vectorize any speech of Barak Obama.\n",
    "#NatGeo- natgeotraveller.in- Make a corpus and find similarity between two articles of a writer, if she is an origialwriter or not.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "'This is the first document.',\n",
    "'This document is the second document.',\n",
    "'And this is the third one.',\n",
    "'Is this the first document?',\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "A = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5898465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928c0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53199211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.nationalgeographic.org/2019/03/08/national-geographics-pristine-seas-team-joins-the-blue-prosperity-coalition/\n",
    "text1 = \"\"\"For the last ten years, the National Geographic Pristine Seasteam has traveled the world to explore, document and protect the last wild places in the ocean. Together with our partners, we have inspired the protection of more than 5 million square kilometers of ocean across 21 areas. But we know that is not enough.\n",
    "\n",
    "Our work is part of the National Geographic Society’s broader vision to protect the natural wonders of life on Earth and move us toward a planet in balance. In order to achieve this goal, we’ve joined with the Wyss Foundation to develop a global Campaign for Nature with the goal of protecting 30 percent of land and 30 percent of our ocean by 2030.\n",
    "\n",
    "A key step toward achieving this goal is working with governments, experts, and other conservation organizations to support the creation of protected areas in the ocean. That’s why we are proud to partner with the Waitt Foundation and the Blue Prosperity Coalition to ensure that the ocean’s last wild places are protected in the long term. The Blue Prosperity Coalitionis a group of organizations with expertise in ocean planning and protected area development that will work with governments to develop sustainable ocean use policies while protecting key areas. By working together, our organizations—the Waitt Institute, National Geographic Pristine Seas, Oceans 5 and Dynamic Planet—can plot a course toward a sustainable ocean future in partnership with governments around the world.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b508d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.nationalgeographic.org/2019/01/31/the-national-geographic-society-joins-with-conservation-organizations-calling-for-an-ambitious-plan-to-safeguard-our-planet/\n",
    "text2 = \"\"\"More than a century ago, National Geographic magazine’s first full-time editor, Gilbert H. Grosvenor, was invited to trek through the Sierra Nevada mountain range. He was so moved by this journey that he devoted the entire edition of the April 1916 issue of the magazine to America’s scenic wonders to encourage citizens to appreciate and preserve the natural world. When it came time to vote on a bill to establish the U.S. National Park Service, Grosvenor delivered copies to every congressman in the Capitol. That same year, 1916, the bill was signed into law.\n",
    "\n",
    "For 131 years, the Society has supported and invested in efforts like Grosvenor’s—efforts that illuminate the wonders of the world, define global challenges, identify solutions, and, perhaps most importantly, inspire action—all with the goal of expanding our understanding of the Earth so we can better protect it for generations.\n",
    "\n",
    "But in that time, the world has changed in profound ways. Threats to the natural world have grown increasingly dire. The facts and figures from the past half century alone are staggering: more than 50 percent of our tropical forests and more than 60 percent of all wildlife populations have been wiped out. Today, 90 percent of the world’s fisheries are exploited or overfished. Our planet is out of balance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8847f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.nationalgeographic.org/2019/01/25/big-cats-initiative-grantees-develop-new-lion-alert-system/\n",
    "text3 = \"\"\"Today, the journal Frontiers of Ecology and Evolution published an analysis of a new lion alert system designed to protect lions and help communities thrive. This innovative approach to promoting human-wildlife coexistence was developed with support from the National Geographic Society’s Big Cats Initiative. Lion populations have plummeted in recent decades due in part to conflict with people. As habitats become more fragmented, lions are more imperiled. In northern Botswana, when lions kill cattle, the villagers often seek revenge by killing lions that may not have been the culprit of the attack.\n",
    "\n",
    "The alert system deploys special satellite tracking collars on lions with a feature called “geofence,” a programmed line of coordinates that gets triggered when the collared animal crosses it and sends a special message to the local community, providing them with a warning about the approaching lion. Villagers that heeded the warnings saw a 50 percent reduction in their livestock losses.\n",
    "\n",
    "“We hope this system will act as a template for alert systems worldwide — whether for wolves on Montana ranchlands, for tigers near villages or for other applications in promoting coexistence and conservation,” said Dr. Andrew Stein, National Geographic Big Cats Initiative grantee and a co-author of the paper.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd0b7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [text1, text2, text3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff28387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1942aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words=stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f242b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix_train = tfidf_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c21d3cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Score : [[1.         0.10189498 0.03633249]]\n"
     ]
    }
   ],
   "source": [
    "print (\"\\nSimilarity Score :\",cosine_similarity(tfidf_matrix_train[0:1], tfidf_matrix_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231747af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c51fdd9d",
   "metadata": {},
   "source": [
    "## Natgeo Traveller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f294ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 1st para of first 5 articles of an author.\n",
    "# Check for the similarity value.If the similarity value is less then that person is a better writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fcd4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b4edf",
   "metadata": {},
   "source": [
    "## Document Extraction and stopword removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5a29013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doc(doc):\n",
    "    f = open(doc,\"r\", encoding='utf-8')\n",
    "    data = f.read()\n",
    "    word_tokens = word_tokenize(data)\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    filtered_sentence = []\n",
    "    stopwords_list = []\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "        else:\n",
    "            stopwords_list.append(w)\n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b76009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://inspiretothrive.com/facebook-pages/\n",
    "d1 = extract_doc(r\"C:\\Users\\Asus\\Desktop\\Jupyter\\NLP\\Task5_articles\\article1.txt\")\n",
    "# https://inspiretothrive.com/guide-to-blogger-outreach-strategy/\n",
    "d2 = extract_doc(r\"C:\\Users\\Asus\\Desktop\\Jupyter\\NLP\\Task5_articles\\article2.txt\")\n",
    "# https://peopledevelopmentmagazine.com/2022/12/28/digital-marketing-initiatives/\n",
    "d3 = extract_doc(r\"C:\\Users\\Asus\\Desktop\\Jupyter\\NLP\\Task5_articles\\article3.txt\")\n",
    "# https://webwriterspotlight.com/content-marketing-trends-to-inform-your-new-year-marketing-strategy\n",
    "d4 = extract_doc(r\"C:\\Users\\Asus\\Desktop\\Jupyter\\NLP\\Task5_articles\\article4.txt\")\n",
    "# https://webwriterspotlight.com/understand-and-plan-your-digital-afterlife\n",
    "d5 = extract_doc(r\"C:\\Users\\Asus\\Desktop\\Jupyter\\NLP\\Task5_articles\\article5.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "626c6eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_words = list(set(d1+d2+d3+d4+d5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa3c15f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['%',\n",
       " 'strong',\n",
       " 'effort',\n",
       " 'account',\n",
       " 'today',\n",
       " 'published',\n",
       " 'Many',\n",
       " 'leads',\n",
       " 'But',\n",
       " 'social',\n",
       " 'high',\n",
       " 'products',\n",
       " 'sense',\n",
       " 'survey',\n",
       " 'us',\n",
       " 'Are',\n",
       " 'profile',\n",
       " 'suggestions',\n",
       " 'business',\n",
       " 'increases',\n",
       " 'established',\n",
       " 'make',\n",
       " 'shopping',\n",
       " 'within',\n",
       " 'goods',\n",
       " 'company',\n",
       " 'results',\n",
       " 'That',\n",
       " 'And',\n",
       " 'findings',\n",
       " 'practical',\n",
       " 'found',\n",
       " 'part',\n",
       " 'asking',\n",
       " 'back',\n",
       " 'However',\n",
       " 'efforts',\n",
       " 'clients',\n",
       " 'reality',\n",
       " 'So',\n",
       " 'optimization',\n",
       " 'This',\n",
       " 'distributing',\n",
       " 'example',\n",
       " 'publish',\n",
       " 'vs.',\n",
       " 'experience',\n",
       " 'outperform',\n",
       " 'peace',\n",
       " 'according',\n",
       " 'illustrate',\n",
       " 'Every',\n",
       " 'mortal',\n",
       " 'sources',\n",
       " 'enduring',\n",
       " 'trail',\n",
       " 'specific',\n",
       " 'traffic',\n",
       " 'Business',\n",
       " 'hour',\n",
       " 'current',\n",
       " 'customers',\n",
       " 'task',\n",
       " 'requires',\n",
       " 'decisions',\n",
       " 'rely',\n",
       " 'website',\n",
       " 'per',\n",
       " 'Consider',\n",
       " 'network',\n",
       " 'understood',\n",
       " 'day',\n",
       " 'campaigns',\n",
       " 'several',\n",
       " 'remains',\n",
       " '?',\n",
       " 'Make',\n",
       " '’',\n",
       " 'consistent',\n",
       " 'Blogger',\n",
       " 'US',\n",
       " 'outreach',\n",
       " 'billion',\n",
       " 'strategy',\n",
       " 'As',\n",
       " '2017',\n",
       " 'page',\n",
       " 'numerous',\n",
       " 'matter',\n",
       " 'potential',\n",
       " 'Page',\n",
       " 'start',\n",
       " 'positive',\n",
       " 'mobilize',\n",
       " 'dependent',\n",
       " '(',\n",
       " 'think',\n",
       " '1.9.',\n",
       " 'lives',\n",
       " 'shuffle',\n",
       " 'best',\n",
       " 'For',\n",
       " 'Keep',\n",
       " 'safe',\n",
       " 'give',\n",
       " 'remember',\n",
       " 'needs',\n",
       " 'shop',\n",
       " 'choosing',\n",
       " 'empowering',\n",
       " 'Q3',\n",
       " 'used',\n",
       " 'real',\n",
       " '2022',\n",
       " 'average',\n",
       " 'Most',\n",
       " 'better',\n",
       " ',',\n",
       " 'past',\n",
       " 'blogger',\n",
       " 'many',\n",
       " 'promote',\n",
       " 'enhance',\n",
       " 'damage',\n",
       " 'difference',\n",
       " 'ability',\n",
       " 'essential',\n",
       " 'also',\n",
       " 'element',\n",
       " 'may',\n",
       " 'content',\n",
       " 'assist',\n",
       " 'coming',\n",
       " 'assets',\n",
       " 'wherever',\n",
       " 'daily',\n",
       " 'engine',\n",
       " 'share',\n",
       " 'reading',\n",
       " 'might',\n",
       " 'emails',\n",
       " 'worldwide',\n",
       " 'target',\n",
       " 'learning',\n",
       " 'holiday',\n",
       " 'You',\n",
       " 'pages',\n",
       " 'site',\n",
       " 'memorialized',\n",
       " 'marketer',\n",
       " 'differently',\n",
       " 'attract',\n",
       " 'They',\n",
       " 'blog',\n",
       " 'indexed',\n",
       " 'niche',\n",
       " 'backlinks',\n",
       " 'topic',\n",
       " 'like',\n",
       " 'importance',\n",
       " 'way',\n",
       " 'I',\n",
       " 'always',\n",
       " 'funeral',\n",
       " 'depends',\n",
       " 'After',\n",
       " '61',\n",
       " 'risks',\n",
       " 'informed',\n",
       " 'market',\n",
       " 'building',\n",
       " 'increase',\n",
       " 'Making',\n",
       " 'almost',\n",
       " '1',\n",
       " 'quality',\n",
       " 'related',\n",
       " 'one',\n",
       " 'deal',\n",
       " 'complex',\n",
       " 'rejections',\n",
       " 'What',\n",
       " 'lack',\n",
       " 'loved',\n",
       " 'mitigate',\n",
       " 'bring',\n",
       " 'Those',\n",
       " 'consumers',\n",
       " 'Maintaining',\n",
       " 'forge',\n",
       " 'plans',\n",
       " 'significant',\n",
       " 'initiatives',\n",
       " 'Facebook',\n",
       " '70',\n",
       " 'prefer',\n",
       " 'specifically',\n",
       " 'hurdles',\n",
       " 'internet',\n",
       " 'become',\n",
       " 'conversions',\n",
       " 'meet',\n",
       " 'If',\n",
       " 'end',\n",
       " 'pleasant',\n",
       " 'help',\n",
       " 'new',\n",
       " 'need',\n",
       " 'required',\n",
       " 'link-building',\n",
       " 'sure-shot',\n",
       " 'would',\n",
       " 'consumer',\n",
       " 'afterlife',\n",
       " 'done',\n",
       " 'SEO',\n",
       " 'assists',\n",
       " 'able',\n",
       " 'via',\n",
       " 'great',\n",
       " 'sites',\n",
       " 'time',\n",
       " 'rise',\n",
       " 'gain',\n",
       " 'Professional',\n",
       " 'pass',\n",
       " 'limiting',\n",
       " 'good',\n",
       " 'arrangements',\n",
       " 'email',\n",
       " 'go',\n",
       " ':',\n",
       " 'strain',\n",
       " 'mortality',\n",
       " 'valuable',\n",
       " '.',\n",
       " 'every',\n",
       " 'including',\n",
       " 'plan',\n",
       " 'result',\n",
       " 'businesses',\n",
       " 'digital',\n",
       " 'landscape',\n",
       " 'presence',\n",
       " 'access',\n",
       " 'developing',\n",
       " 'taking',\n",
       " 'technologies',\n",
       " 'high-quality',\n",
       " 'away',\n",
       " 'virtually',\n",
       " 'organizations',\n",
       " 'marketers',\n",
       " 'areas',\n",
       " 'unpublishing',\n",
       " 'credibility',\n",
       " 'evaluate',\n",
       " 'garbage',\n",
       " 'support',\n",
       " 'recently',\n",
       " 'audience',\n",
       " 'generating',\n",
       " 'Look',\n",
       " 'years',\n",
       " 'messages',\n",
       " 'overlooked',\n",
       " 'demanding',\n",
       " 'packing',\n",
       " 'informational',\n",
       " 'write',\n",
       " 'online',\n",
       " 'helps',\n",
       " 'removing',\n",
       " 'trust',\n",
       " 'time-consuming',\n",
       " 'prepare',\n",
       " 'built',\n",
       " 'overcome',\n",
       " 'perfectly',\n",
       " 'guided',\n",
       " 'idea',\n",
       " 'spent',\n",
       " 'success',\n",
       " 'The',\n",
       " 'adapting',\n",
       " 'something',\n",
       " 'More',\n",
       " 'people',\n",
       " 'legal',\n",
       " 'value',\n",
       " 'services',\n",
       " 'position',\n",
       " 'life',\n",
       " 'trend',\n",
       " 'search',\n",
       " 'It',\n",
       " 'necessity',\n",
       " 'popular',\n",
       " 'How',\n",
       " 'ones',\n",
       " 'probably',\n",
       " 'relevance',\n",
       " 'Pages',\n",
       " 'engines',\n",
       " 'competition',\n",
       " 'directly',\n",
       " 'media',\n",
       " 'linking',\n",
       " 'One',\n",
       " 'significantly',\n",
       " 'necessities',\n",
       " 'research',\n",
       " 'majority',\n",
       " 'Will',\n",
       " ')',\n",
       " 'industry',\n",
       " 'challenge',\n",
       " 'brand',\n",
       " 'fact',\n",
       " 'making',\n",
       " 'LOT',\n",
       " 'flood',\n",
       " 'connect',\n",
       " 'unwind',\n",
       " 'variety',\n",
       " 'personal',\n",
       " 'fits',\n",
       " '77',\n",
       " 'year',\n",
       " 'scoff',\n",
       " 'number',\n",
       " 'spend',\n",
       " 'According',\n",
       " 'stories',\n",
       " 'integrated',\n",
       " 'brands',\n",
       " 'got',\n",
       " 'advanced',\n",
       " 'awareness',\n",
       " 'mind',\n",
       " 'credible',\n",
       " 'bonds',\n",
       " 'users',\n",
       " 'take',\n",
       " 'marketing',\n",
       " 'While',\n",
       " 'understand',\n",
       " 'A',\n",
       " 'change',\n",
       " 'leave',\n",
       " 'considering',\n",
       " 'coil',\n",
       " 'ignore',\n",
       " 'profiles',\n",
       " 'right',\n",
       " 'accounts',\n",
       " 'win',\n",
       " 'left',\n",
       " 'There',\n",
       " 'maximizing',\n",
       " 'active',\n",
       " 'often',\n",
       " 'In',\n",
       " 'factors',\n",
       " 'realtors',\n",
       " 'approach',\n",
       " 'upcoming',\n",
       " 'data',\n",
       " 'next',\n",
       " 'targeting',\n",
       " 'image',\n",
       " 'works']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b2bd7",
   "metadata": {},
   "source": [
    "## Bag of words representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84a3f08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      words  d1  d2  d3  d4  d5\n",
      "0         %   0   2   0   2   0\n",
      "1    strong   0   0   1   0   0\n",
      "2    effort   0   0   1   0   0\n",
      "3   account   1   0   0   0   0\n",
      "4     today   1   0   0   0   0\n",
      "..      ...  ..  ..  ..  ..  ..\n",
      "95        (   1   1   1   0   0\n",
      "96    think   0   0   0   1   1\n",
      "97     1.9.   1   0   0   0   0\n",
      "98    lives   0   0   0   0   1\n",
      "99  shuffle   0   0   0   0   1\n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def Bag_of_words(word_list):\n",
    "    word_l = []\n",
    "    for word in distinct_words:\n",
    "        if (word in word_list) == True:\n",
    "            word_l.append(word_list.count(word))\n",
    "        else:\n",
    "            word_l.append(0)\n",
    "        \n",
    "    return word_l\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"words\"] = distinct_words\n",
    "df[\"d1\"] = Bag_of_words(d1)\n",
    "df[\"d2\"] = Bag_of_words(d2)\n",
    "df[\"d3\"] = Bag_of_words(d3)\n",
    "df[\"d4\"] = Bag_of_words(d4)\n",
    "df[\"d5\"] = Bag_of_words(d5)\n",
    "    \n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fde18f",
   "metadata": {},
   "source": [
    "## TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989eaa8f",
   "metadata": {},
   "source": [
    "### tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fa7946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         words   d1        d2   d3        d4   d5\n",
      "0            %  0.0  1.526589  0.0  1.526589  0.0\n",
      "1       strong  0.0  0.000000  1.0  0.000000  0.0\n",
      "2       effort  0.0  0.000000  1.0  0.000000  0.0\n",
      "3      account  1.0  0.000000  0.0  0.000000  0.0\n",
      "4        today  1.0  0.000000  0.0  0.000000  0.0\n",
      "..         ...  ...       ...  ...       ...  ...\n",
      "367       data  0.0  0.000000  0.0  1.526589  1.0\n",
      "368       next  0.0  0.000000  0.0  1.000000  0.0\n",
      "369  targeting  0.0  0.000000  0.0  1.000000  0.0\n",
      "370      image  0.0  0.000000  1.0  0.000000  0.0\n",
      "371      works  0.0  1.000000  0.0  0.000000  0.0\n",
      "\n",
      "[372 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def TF(word_list):\n",
    "    word_l = []\n",
    "    for word in distinct_words:\n",
    "        if (word in word_list) == True:\n",
    "            word_l.append(1+math.log(1+math.log(word_list.count(word))))\n",
    "        else:\n",
    "            word_l.append(0)\n",
    "        \n",
    "    return word_l\n",
    "\n",
    "tf = pd.DataFrame()\n",
    "tf[\"words\"] = distinct_words\n",
    "tf[\"d1\"] = TF(d1)\n",
    "tf[\"d2\"] = TF(d2)\n",
    "tf[\"d3\"] = TF(d3)\n",
    "tf[\"d4\"] = TF(d4)\n",
    "tf[\"d5\"] = TF(d5)\n",
    "    \n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be355e",
   "metadata": {},
   "source": [
    "### idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59e4ac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word  relevance\n",
      "0            %   1.252763\n",
      "1       strong   1.791759\n",
      "2       effort   1.791759\n",
      "3      account   1.791759\n",
      "4        today   1.791759\n",
      "..         ...        ...\n",
      "367       data   1.252763\n",
      "368       next   1.791759\n",
      "369  targeting   1.791759\n",
      "370      image   1.791759\n",
      "371      works   1.791759\n",
      "\n",
      "[372 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "idf = pd.DataFrame()\n",
    "\n",
    "def IDF():\n",
    "    l = []\n",
    "    D = [d1,d2,d3,d4,d5]\n",
    "    for i in distinct_words:\n",
    "        d = 0\n",
    "        for j in D:\n",
    "            if i in j:\n",
    "                d+=1\n",
    "                \n",
    "        l.append(math.log(1+5/d))\n",
    "        \n",
    "    return l\n",
    "        \n",
    "idf[\"word\"] = distinct_words\n",
    "idf[\"relevance\"] = IDF()\n",
    "\n",
    "        \n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c32fc",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fddb7c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         words        d1        d2        d3        d4        d5\n",
      "0            %  0.000000  1.912454  0.000000  1.912454  0.000000\n",
      "1       strong  0.000000  0.000000  1.791759  0.000000  0.000000\n",
      "2       effort  0.000000  0.000000  1.791759  0.000000  0.000000\n",
      "3      account  1.791759  0.000000  0.000000  0.000000  0.000000\n",
      "4        today  1.791759  0.000000  0.000000  0.000000  0.000000\n",
      "..         ...       ...       ...       ...       ...       ...\n",
      "367       data  0.000000  0.000000  0.000000  1.912454  1.252763\n",
      "368       next  0.000000  0.000000  0.000000  1.791759  0.000000\n",
      "369  targeting  0.000000  0.000000  0.000000  1.791759  0.000000\n",
      "370      image  0.000000  0.000000  1.791759  0.000000  0.000000\n",
      "371      works  0.000000  1.791759  0.000000  0.000000  0.000000\n",
      "\n",
      "[372 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tfidf = pd.DataFrame()\n",
    "tfidf[\"words\"] = distinct_words\n",
    "\n",
    "n1 = np.array(tf.d1)\n",
    "n2 = np.array(tf.d2)\n",
    "n3 = np.array(tf.d3)\n",
    "n4 = np.array(tf.d4)\n",
    "n5 = np.array(tf.d5)\n",
    "x = np.array(idf.relevance)\n",
    "\n",
    "tfidf[\"d1\"] = n1*x\n",
    "tfidf[\"d2\"] = n2*x\n",
    "tfidf[\"d3\"] = n3*x\n",
    "tfidf[\"d4\"] = n4*x\n",
    "tfidf[\"d5\"] = n5*x\n",
    "\n",
    "print(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04adf8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         words        d1        d2        d3        d4        d5\n",
      "0            %  0.000000  0.005141  0.000000  0.005141  0.000000\n",
      "1       strong  0.000000  0.000000  0.004817  0.000000  0.000000\n",
      "2       effort  0.000000  0.000000  0.004817  0.000000  0.000000\n",
      "3      account  0.004817  0.000000  0.000000  0.000000  0.000000\n",
      "4        today  0.004817  0.000000  0.000000  0.000000  0.000000\n",
      "..         ...       ...       ...       ...       ...       ...\n",
      "367       data  0.000000  0.000000  0.000000  0.005141  0.003368\n",
      "368       next  0.000000  0.000000  0.000000  0.004817  0.000000\n",
      "369  targeting  0.000000  0.000000  0.000000  0.004817  0.000000\n",
      "370      image  0.000000  0.000000  0.004817  0.000000  0.000000\n",
      "371      works  0.000000  0.004817  0.000000  0.000000  0.000000\n",
      "\n",
      "[372 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#normalized tf-idf for docs\n",
    "tfidf.d1/=len(distinct_words)\n",
    "tfidf.d2/=len(distinct_words)\n",
    "tfidf.d3/=len(distinct_words)\n",
    "tfidf.d4/=len(distinct_words)\n",
    "tfidf.d5/=len(distinct_words)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195d3337",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7479f31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d1': 0.0759332963, 'd2': 0.0437981883, 'd3': 0.0439621046, 'd4': 0.1003604655}\n"
     ]
    }
   ],
   "source": [
    "#cosine-similarity\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "cs = {}\n",
    "q = 0\n",
    "for i in np.array(tfidf.d5):\n",
    "    q+=(i**2)\n",
    "    \n",
    "# print(q)\n",
    "\n",
    "def cosine_sim(doc):\n",
    "    d = 0\n",
    "    for i in doc:\n",
    "        d+=(i**2) \n",
    "#     print(d)\n",
    "    a = doc.dot(np.array(tfidf.d5))\n",
    "    return('{0:.10f}'.format((a/(math.sqrt(q*d)))))\n",
    "\n",
    "cs[\"d1\"] = float(cosine_sim(np.array(tfidf.d1)))\n",
    "cs[\"d2\"] = float(cosine_sim(np.array(tfidf.d2)))\n",
    "cs[\"d3\"] = float(cosine_sim(np.array(tfidf.d3)))\n",
    "cs[\"d4\"] = float(cosine_sim(np.array(tfidf.d4)))\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecbbc79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#document ranking\n",
    "def rank_doc(array):\n",
    "    l = array.copy()\n",
    "    valid = True\n",
    "    while valid:\n",
    "#         print(l)\n",
    "        if(len(np.where(array == max(l))[0]) > 1):\n",
    "            for i in range(len(np.where(array == max(l))[0])):\n",
    "                print((np.where(array == max(l))[0][i]+1),)\n",
    "            l = np.delete(l,np.where(l == max(l)),0)\n",
    "        else:\n",
    "            print(np.where(array == max(l))[0][0]+1)\n",
    "            l = np.delete(l,np.where(l == max(l))[0],0)\n",
    "            \n",
    "        if(len(l) == 0):\n",
    "            valid = False\n",
    "            \n",
    "\n",
    "rank_doc(np.array(list(cs.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd434c",
   "metadata": {},
   "source": [
    "## Eucledian Distance and Document Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6651a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d1': 0.0610668697, 'd2': 0.0660151112, 'd3': 0.0613658425, 'd4': 0.0619431864}\n",
      "[0.0610668697, 0.0660151112, 0.0613658425, 0.0619431864]\n",
      "1\n",
      "3\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "ed = {}\n",
    "def euc_d(doc,query):\n",
    "    e = 0\n",
    "    for i in range(len(doc)):\n",
    "        e+=(doc[i] - query[i])**2\n",
    "        \n",
    "    return ('{0:.10f}'.format(math.sqrt(e)))\n",
    "        \n",
    "ed[\"d1\"] = float(euc_d(np.array(tfidf.d1), np.array(tfidf.d5)))\n",
    "ed[\"d2\"] = float(euc_d(np.array(tfidf.d2), np.array(tfidf.d5)))\n",
    "ed[\"d3\"] = float(euc_d(np.array(tfidf.d3), np.array(tfidf.d5)))\n",
    "ed[\"d4\"] = float(euc_d(np.array(tfidf.d4), np.array(tfidf.d5)))\n",
    "print(ed)\n",
    "print(list(ed.values()))\n",
    "\n",
    "def rank_doc(array):\n",
    "    l = array.copy()\n",
    "    valid = True\n",
    "    while valid:\n",
    "        if(len(np.where(array == min(l))[0]) > 1):\n",
    "            for i in range(len(np.where(array == min(l))[0])):\n",
    "                print((np.where(array == min(l))[0][i]+1),)\n",
    "            l = np.delete(l,np.where(l == min(l)),0)\n",
    "        else:\n",
    "            print(np.where(array == min(l))[0][0]+1)\n",
    "            l = np.delete(l,np.where(l == min(l))[0],0)\n",
    "            \n",
    "        if(len(l) == 0):\n",
    "            valid = False\n",
    "\n",
    "            \n",
    "\n",
    "rank_doc(np.array(list(ed.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bfb49b",
   "metadata": {},
   "source": [
    "Final Note - we see that after computing the cosine similarity and Eucledian distance between the 5th article and the other 4 articles are negligible. \n",
    "So we can say that the articles she writes are original and we can hire her for our publication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a0ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
